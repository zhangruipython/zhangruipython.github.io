<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhangruipython.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="我的学习记录">
<meta property="og:type" content="website">
<meta property="og:title" content="MyBlog">
<meta property="og:url" content="https://zhangruipython.github.io/index.html">
<meta property="og:site_name" content="MyBlog">
<meta property="og:description" content="我的学习记录">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="张睿">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://zhangruipython.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>MyBlog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">MyBlog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhangruipython.github.io/2021/10/29/sedona%E5%A4%84%E7%90%86%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%9C%B0%E7%90%86%E6%95%B0%E6%8D%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张睿">
      <meta itemprop="description" content="我的学习记录">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MyBlog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/10/29/sedona%E5%A4%84%E7%90%86%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%9C%B0%E7%90%86%E6%95%B0%E6%8D%AE/" class="post-title-link" itemprop="url">sedona处理大规模地理数据 </a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-10-29 14:52:53 / 修改时间：14:53:58" itemprop="dateCreated datePublished" datetime="2021-10-29T14:52:53+08:00">2021-10-29</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p> <a target="_blank" rel="noopener" href="http://sedona.apache.org/">Sedona</a>是一个基于spark的集群计算框架，可以大规模处理地理空间数据。Sedona扩展了Spark中数据结构RDD，使之可以兼容集群中地理空间数据，Sedona的SpatialRDD可以由普通RDD转换而来或者通过读取数据源而生成。<br>Sedona提供了开箱即用的空间SQL API和RDD API，SQL API 标准与PostgreSQL一致，RDD API支持Scala，Java，Python，R语言开发。</p>
<h2 id="Sedona-支持读取的数据源"><a href="#Sedona-支持读取的数据源" class="headerlink" title="Sedona 支持读取的数据源"></a>Sedona 支持读取的数据源</h2><blockquote>
<p>”spark 支持读取的数据源，Sedona都支持“</p>
</blockquote>
<blockquote>
<p>1、<code>CSV</code><br>2、<code>HDFS</code><br>3、<code>Amazon S3</code><br>4、<code>ODPS</code></p>
</blockquote>
<h2 id="Sedona-支持解析的地理数据格式"><a href="#Sedona-支持解析的地理数据格式" class="headerlink" title="Sedona 支持解析的地理数据格式"></a>Sedona 支持解析的地理数据格式</h2><p>1、<code>WKT</code><br>2、<code>WKB</code><br>3、<code>GeoJSON</code><br>4、<code>Shapefile</code></p>
<p><strong>Sedona</strong> 内部的 Spatial RDD兼容7种空间类型:</p>
<ul>
<li>点</li>
<li>多点</li>
<li>多边形</li>
<li>多个多边形</li>
<li>线条</li>
<li>多个线条</li>
<li>圆形</li>
<li>Geometry集合</li>
</ul>
<h2 id="Sedona-处理大规模数据集的方式"><a href="#Sedona-处理大规模数据集的方式" class="headerlink" title="Sedona 处理大规模数据集的方式"></a>Sedona 处理大规模数据集的方式</h2><h3 id="空间数据分区"><a href="#空间数据分区" class="headerlink" title="空间数据分区"></a>空间数据分区</h3><p>Sedona 中的Spatial RDD中的数据根据空间数据分布进行分区，附近的空间的对象会被放在同一分区内。空间分区具有两种效果：</p>
<p>（1）当执行针对特定空间区域的空间查询时，可以避免对空间不接近的分区进行不必要的计算，从而加快查询速度。</p>
<p>（2）将Spatial RDD切分为多个数据分区，每个分区的数据量接近，这样在集群计算时可以避免出现数据倾斜。</p>
<p>Sedona目前支持的空间分区模式有三种：</p>
<ul>
<li>KDB-Tree,</li>
<li>Quad-Tree </li>
<li>R-Tree</li>
</ul>
<p><img src="https://i.loli.net/2021/06/02/N7LHk3afsl2VpuD.png" alt="1_wdriTIbaf1b6EME7O0QbKg.png"></p>
<h3 id="空间数据索引"><a href="#空间数据索引" class="headerlink" title="空间数据索引"></a>空间数据索引</h3><p>Sedona 使用分布式空间索引对集群中的Spatial RDD进行索引，Sedona的分布式索引由两部分组成</p>
<p>（1）global index 全局索引，存储在master机器上，在spatial rdd 进行空间分区阶段生成，global index定位Spatial RDD中的空间分区边界框，global index 的目的是在空间查询时快速去除确定没有有效空间对象的空间分区。</p>
<p>（2）local index 本地索引，建立在Spatial RDD 的每一个空间分区里面，每一个local index只对自己空间分区的数据有作用，在空间查询中local index可以并行计算用于加速空间查询。</p>
<h3 id="Spatial-RDD定制化的序列化方式"><a href="#Spatial-RDD定制化的序列化方式" class="headerlink" title="Spatial RDD定制化的序列化方式"></a>Spatial RDD定制化的序列化方式</h3><p>Sedona针对空间对象和空间索引提供了定制化的序列化方式，Sedona序列化器可以将空间对象和空间索引序列化为压缩的字节数组，该序列化器比spark中常用的kyro序列化更快，在进行复杂空间操作（例如空间连接查询）时，占用内存较小。<br> 序列化器还可以序列化和反序列化local index ，使用DFS（深度优先遍历）先父节点再写对应的子节点。</p>
<h2 id="Sedona地理空间计算和JDBC连接PostgresSQL对比"><a href="#Sedona地理空间计算和JDBC连接PostgresSQL对比" class="headerlink" title="Sedona地理空间计算和JDBC连接PostgresSQL对比"></a>Sedona地理空间计算和JDBC连接PostgresSQL对比</h2><h3 id="计算效率对比"><a href="#计算效率对比" class="headerlink" title="计算效率对比"></a>计算效率对比</h3><ul>
<li>两种方案统一的Spark 集群配置都是60 instance , 2g instance memory, 2 instance core</li>
<li>Sedona使用KDB-Tree作为空间分区算法，使用Quad-Tree作为空间索引算法</li>
<li>PostgresSQL基于一台120核机器计算</li>
</ul>
<table>
<thead>
<tr>
<th>方案名称</th>
<th>计算数据量</th>
<th>计算内容</th>
<th>耗时</th>
<th>TPS</th>
</tr>
</thead>
<tbody><tr>
<td>分布式JDBC连接PostgresSQL地理计算</td>
<td>449655129条数据</td>
<td>计算5月份全量数据经纬度对应的路况</td>
<td>20h40m</td>
<td>6044</td>
</tr>
<tr>
<td>Spark+Sedona地理计算</td>
<td>449655129条数据</td>
<td>计算5月份全量数据经纬度对应的路况</td>
<td>2h20m2s</td>
<td>53518</td>
</tr>
</tbody></table>
<h3 id="计算精准度对比"><a href="#计算精准度对比" class="headerlink" title="计算精准度对比"></a>计算精准度对比</h3><p>Spark+Sedona地理计算方案的计算结果有87%的结果数据可以和PostgresSQL计算结果匹配上</p>
<h2 id="使用Sedona-RDD-API进行查询"><a href="#使用Sedona-RDD-API进行查询" class="headerlink" title="使用Sedona RDD API进行查询"></a>使用Sedona RDD API进行查询</h2><h3 id="配置依赖"><a href="#配置依赖" class="headerlink" title="配置依赖"></a>配置依赖</h3><p>Sedona版本和spark版本保持一致<br><img src="/uploads/a8325f09f43a30a0d734448e42d8f0c3/image.png" alt="image"></p>
<h3 id="初始化spark上下文"><a href="#初始化spark上下文" class="headerlink" title="初始化spark上下文"></a>初始化spark上下文</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val conf = new SparkConf()</span><br><span class="line">conf.setAppName(“GeoSparkExample”)</span><br><span class="line">conf.set(“spark.serializer”, classOf[KryoSerializer].getName)</span><br><span class="line">conf.set(“spark.kryo.registrator”, classOf[GeoSparkKryoRegistrator].getName)</span><br><span class="line">val sc = new SparkContext(conf)</span><br></pre></td></tr></table></figure>

<h3 id="创建SpatialRdd"><a href="#创建SpatialRdd" class="headerlink" title="创建SpatialRdd"></a>创建SpatialRdd</h3><p>将spark dataframe转为SpatialRdd，并指定Geometry类型字段</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val roadRDD = Adapter.toSpatialRdd(roadWktDF, &quot;geom_wkt&quot;, Seq(&quot;ref&quot;, &quot;fclass&quot;))</span><br></pre></td></tr></table></figure>

<h3 id="构建空间索引"><a href="#构建空间索引" class="headerlink" title="构建空间索引"></a>构建空间索引</h3><p>可以在SpatialRDD上构建分布式空间索引。目前，该系统提供两种类型的空间索引，QUADTREE和RTREE，作为每个分区的本地索引</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spatialRDD.buildIndex(IndexType.QUADTREE, false) // 在进行空间join查询时设置为true</span><br></pre></td></tr></table></figure>

<h3 id="编写空间范围查询"><a href="#编写空间范围查询" class="headerlink" title="编写空间范围查询"></a>编写空间范围查询</h3><p>空间范围查询返回位于地理区域内的所有空间对象。<br>例如，范围查询可能会在余杭找到所有公园，或在用户当前位置的一英里范围内返回所有公园。在入参方面，空间范围查询以一组空间对象和一个多边形查询窗口作为输入，并返回位于查询区域的所有空间对象。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rangeQueryWindow = new Envelope(-90.01, -80.01, 30.01, 40.01) //定义矩形空间窗口</span><br><span class="line">/* considerIntersect=true, 只返回被窗口完全覆盖的Geometry; considerIntersect=false，返回和窗口发生相交的Geometry */</span><br><span class="line">val considerIntersect = false</span><br><span class="line">val usingIndex = false</span><br><span class="line">var queryResult = RangeQuery.SpatialRangeQuery(spatialRDD, rangeQueryWindow, considerIntersect, usingIndex)</span><br></pre></td></tr></table></figure>

<h3 id="编写K近邻查询"><a href="#编写K近邻查询" class="headerlink" title="编写K近邻查询"></a>编写K近邻查询</h3><p>输入 K、查询点和空间RDD集合，查询距离查询点最近的K个空间RDD</p>
<h3 id="编写空间连接查询"><a href="#编写空间连接查询" class="headerlink" title="编写空间连接查询"></a>编写空间连接查询</h3><p>空间连接查询是将两个或多个数据集与空间距离相结合的查询，例如查询在500KM范围内有杂货店的加油站。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val considerBoundaryIntersection = true</span><br><span class="line">val usingIndex = true</span><br><span class="line">val resultRdd = DistanceJoinQueryFlat(roadRDD, pointCircleRDD, usingIndex, considerBoundaryIntersection)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhangruipython.github.io/2020/07/15/Hive-UDF%E6%9E%84%E5%BB%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张睿">
      <meta itemprop="description" content="我的学习记录">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MyBlog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/15/Hive-UDF%E6%9E%84%E5%BB%BA/" class="post-title-link" itemprop="url">Hive UDF构建</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-15 15:48:34" itemprop="dateCreated datePublished" datetime="2020-07-15T15:48:34+08:00">2020-07-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-10-29 15:05:45" itemprop="dateModified" datetime="2021-10-29T15:05:45+08:00">2021-10-29</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Hive-UDF-构建"><a href="#Hive-UDF-构建" class="headerlink" title="Hive UDF 构建"></a>Hive UDF 构建</h2><h3 id="什么是Hive-UDF"><a href="#什么是Hive-UDF" class="headerlink" title="什么是Hive UDF"></a>什么是Hive UDF</h3><p>UDF（User-Defined-Functions）用户自定义的Hive函数。</p>
<h3 id="Hive-UDF-种类"><a href="#Hive-UDF-种类" class="headerlink" title="Hive UDF 种类"></a>Hive UDF 种类</h3><ul>
<li>UDF：one to one ，操作单个数据行，产生对应的单行数据</li>
<li>UDAF：many to one，操作多行数据，产生一行数据</li>
<li>UDTF：one to many，操作一行数据，产生多行数据</li>
</ul>
<h3 id="实现UDF"><a href="#实现UDF" class="headerlink" title="实现UDF"></a>实现UDF</h3><h4 id="maven-依赖"><a href="#maven-依赖" class="headerlink" title="maven 依赖"></a>maven 依赖</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;3.1.2&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;3.2.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<h4 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h4><ul>
<li><p>hive版本和Hadoop版本一定要对应，不然会产生jar包冲突</p>
</li>
<li><p>UDF实现类需要继承 GenericUDF，许多教程案列中写的是继承UDF，在hive 3.1.2版本中 UDF类已经被标记为@Deprecated，不建议使用</p>
</li>
</ul>
<h4 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h4><ul>
<li>继承GenericUDF后需要复写三个方法，分别是initialize，evaluate，getDisplayString</li>
<li>initialize方法在UDF实现类中首先被调用，主要负责：<ul>
<li>参数校验，验证输入参数类型是否符合预期</li>
<li>设置返回值，设置返回一个与预期输出类型相符合的对象</li>
<li>存储全局变量，为全局变量赋值</li>
</ul>
</li>
<li>evaluate方法处理具体逻辑，返回预期执行结果</li>
<li>getDisplayString：类似toString方法</li>
</ul>
<h4 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">package org.example.util;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.exec.Description;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;</span><br><span class="line">import org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;</span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.NDV;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;</span><br><span class="line">import org.apache.hadoop.io.BooleanWritable;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * 编写hive udf</span><br><span class="line"> * @author 张睿</span><br><span class="line"> * @create 2020-07-15 10:16</span><br><span class="line"> **/</span><br><span class="line">@Description(name = &quot;judge&quot;,value = &quot;_FUNC_(array,value) - Returns TRUE if the array contains value&quot;</span><br><span class="line">        ,extended = &quot;Example: SELECT _FUNC_(array(&#x27;a&#x27;,&#x27;b&#x27;),&#x27;a&#x27;)&quot;)</span><br><span class="line">@NDV(maxNdv = 2)</span><br><span class="line">public class ComplexUdf extends GenericUDF&#123;</span><br><span class="line">    ListObjectInspector listOI;</span><br><span class="line">    ObjectInspector elementOI;</span><br><span class="line">    </span><br><span class="line">    @Override</span><br><span class="line">    public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException &#123;</span><br><span class="line">        // 参数数量校验</span><br><span class="line">        if(arguments.length!=2)&#123;</span><br><span class="line">            throw new UDFArgumentLengthException(&quot;the operator accept two arguments&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        // 参数类型校验</span><br><span class="line">        ObjectInspector a = arguments[0];</span><br><span class="line">        ObjectInspector b = arguments[1];</span><br><span class="line">        if(!(a instanceof ListObjectInspector)||!(b instanceof StringObjectInspector))&#123;</span><br><span class="line">            throw new UDFArgumentException(&quot;first argument must be a list / array, second argument must be a string&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        this.listOI = (ListObjectInspector) a;</span><br><span class="line">        this.elementOI =  b;</span><br><span class="line">        // 校验list是否由string组成</span><br><span class="line">        if(!(listOI.getListElementObjectInspector() instanceof StringObjectInspector))&#123;</span><br><span class="line">            throw new UDFArgumentException(&quot;first argument must be a list of strings&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        return PrimitiveObjectInspectorFactory.writableBooleanObjectInspector;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    @Override</span><br><span class="line">    public Object evaluate(DeferredObject[] deferredObjects) throws HiveException &#123;</span><br><span class="line">        // 使用对象检查器从延迟对象中获取列表和字符串</span><br><span class="line">        BooleanWritable result = new BooleanWritable(false);</span><br><span class="line">        int elemNum = this.listOI.getListLength(deferredObjects[0].get());</span><br><span class="line">        Object arg = deferredObjects[1].get();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        for(int i=0;i&lt;elemNum;i++)&#123;</span><br><span class="line">		   this.listOI.getListElement(deferredObjects[0].get(),i);</span><br><span class="line">            Object listElement = listOI.getListElement(deferredObjects[0].get(),i);</span><br><span class="line">//            String element = elementOI.getPrimitiveJavaObject(lazyString);</span><br><span class="line">            if (arg.equals(listElement))&#123;</span><br><span class="line">                result.set(true);</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return result;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public String getDisplayString(String[] args) &#123;</span><br><span class="line">        return &quot;if&quot;+args[0]+&quot; include &quot;+args[1]+&quot; return true&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="注册UDF函数"><a href="#注册UDF函数" class="headerlink" title="注册UDF函数"></a>注册UDF函数</h3><ul>
<li>上传本地jar包：add jar xxx; (xxx对应jar包本地路径)</li>
<li>注册临时函数：create temporary function args01 as “args02”; (args01:UDF方法名称，args02:具体实现类路径)</li>
<li>注册永久函数：create function args01 as “args02”;</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhangruipython.github.io/2020/07/07/%E4%BD%BF%E7%94%A8redis-presto%E5%AE%9E%E7%8E%B0rocksdb%E4%B8%AD%E6%95%B0%E6%8D%AESQL%E7%BB%93%E6%9E%84%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张睿">
      <meta itemprop="description" content="我的学习记录">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MyBlog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/07/%E4%BD%BF%E7%94%A8redis-presto%E5%AE%9E%E7%8E%B0rocksdb%E4%B8%AD%E6%95%B0%E6%8D%AESQL%E7%BB%93%E6%9E%84%E5%8C%96/" class="post-title-link" itemprop="url">使用redis+presto实现RocksDB中数据SQL结构化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-07-07 16:06:05 / 修改时间：17:30:00" itemprop="dateCreated datePublished" datetime="2020-07-07T16:06:05+08:00">2020-07-07</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="通过使用presto结合Redis实现RocksDB数据SQL结构化"><a href="#通过使用presto结合Redis实现RocksDB数据SQL结构化" class="headerlink" title="通过使用presto结合Redis实现RocksDB数据SQL结构化"></a>通过使用presto结合Redis实现RocksDB数据SQL结构化</h2><h3 id="为什么使用presto结合redis的方式实现？"><a href="#为什么使用presto结合redis的方式实现？" class="headerlink" title="为什么使用presto结合redis的方式实现？"></a>为什么使用presto结合redis的方式实现？</h3><ul>
<li>一开始我的目标是：因为presto支持自定义数据源插件开发，所以自己实现以RocksDB 作为数据源的插件，但是在开发过程中发现，因为RocksDB 数据源与业务耦合太紧，许多操作需要定制化开发，例如数据解密、数据反序列化、metadata映射等问题，这一步很难通过读取外部的配置文件或者通过presto客户端传递参数实现。所以我想借助第三方数据中间件进行业务与功能的去耦合。</li>
</ul>
<h3 id="实现以redis作为数据源，presto作为即席查询分析"><a href="#实现以redis作为数据源，presto作为即席查询分析" class="headerlink" title="实现以redis作为数据源，presto作为即席查询分析"></a>实现以redis作为数据源，presto作为即席查询分析</h3><h4 id="什么是即席查询分析"><a href="#什么是即席查询分析" class="headerlink" title="什么是即席查询分析"></a>什么是即席查询分析</h4><ul>
<li>即席查询分析（Ad Hoc）：用户根据自身需求灵活选择查询条件，即席查询与普通应用查询最大的不同是普通的应用查询是定制开发的，而即席查询是由用户自定义查询条件的。 </li>
<li>即席查询和分析的计算模式兼具了良好的时效性与灵活性，是对批处理，流计算两大计算模式有力补充。</li>
</ul>
<h4 id="开发步骤"><a href="#开发步骤" class="headerlink" title="开发步骤"></a>开发步骤</h4><ul>
<li>在presto中配置redis有关参数，在etc&#x2F;catalog&#x2F;目录下新建redis.properties 配置如下：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 连接名称</span><br><span class="line">connector.name=redis</span><br><span class="line"># redis服务端节点地址</span><br><span class="line">redis.nodes=192.168.1.54:6379</span><br><span class="line"># redis中表名称</span><br><span class="line">redis.table-names=user_info</span><br><span class="line"># redis中库名称</span><br><span class="line">redis.default-schema=user_profile</span><br><span class="line"># redis metadata配置文件路径</span><br><span class="line">redis.table-description-dir=/home/Documents/presto-server-0.235.1/etc/catalog/redis</span><br><span class="line"># presto查找表名称与库名称分隔符</span><br><span class="line">redis.key-delimiter=:</span><br><span class="line">redis.key-prefix-schema-table=true</span><br><span class="line">redis.hide-internal-columns=false</span><br><span class="line">redis.hide-internal-columns=false</span><br></pre></td></tr></table></figure>

<p><strong>注意点</strong>：因为在redis中存储数据结构为 hash，所以presto通过解析 key值生成结构化数据中的表名称和库名称，解析的分隔符默认为：</p>
<ul>
<li>创建redis metadata文件，在redis.properties 中写定的metadata配置路径下新建redis.json文件，文件如下：</li>
</ul>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;tableName&quot;</span><span class="punctuation">:</span> <span class="string">&quot;user_info&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;schemaName&quot;</span><span class="punctuation">:</span> <span class="string">&quot;user_profile&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;key&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;dataFormat&quot;</span><span class="punctuation">:</span> <span class="string">&quot;raw&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;fields&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;redis_key&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;varchar&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;hidden&quot;</span><span class="punctuation">:</span> <span class="string">&quot;false&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;dataFormat&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hash&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;fields&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hospitalizedNum&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;varchar&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;mapping&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hospitalizedNum&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span><span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hid&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;varchar&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;mapping&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hid&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>



<ul>
<li>在redis客户端中写入数据，数据格式为HASH，进行测试：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>在presto客户端进行查询：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./presto --server localhost:8080 --catalog redis --schema user_profile（user_profile即为redis key:分隔符第一位）</span><br></pre></td></tr></table></figure>

<ul>
<li>结果展示：</li>
</ul>
<p><img src="https://i.loli.net/2020/07/07/A1Yh6c5Hvgpib9L.png" alt="批注 2020-07-07 170252.png"></p>
<h3 id="RocksDB中大批次数据写入Redis中"><a href="#RocksDB中大批次数据写入Redis中" class="headerlink" title="RocksDB中大批次数据写入Redis中"></a>RocksDB中大批次数据写入Redis中</h3><h4 id="使用redis-pipeline提高写入速度"><a href="#使用redis-pipeline提高写入速度" class="headerlink" title="使用redis pipeline提高写入速度"></a>使用redis pipeline提高写入速度</h4><p>Redis是使用客户端-服务端模型和请求&#x2F;响应协议的TCP服务器，通常情况下一次请求需要以下步骤</p>
<p>1、客户端向服务端发送查询，以阻塞的方式从套接字中读取服务器响应</p>
<p>2、服务器处理命令并将响应发送给客户端</p>
<p>这种模式依赖于RTT（往返时间），如果有1000条数据插入，则会耗时1000*RTT</p>
<p>pipeline是将所有命令打包，通过一次网络参数发送至服务端，所以可以大大减少网络通信时间。</p>
<h4 id="使用多线程处理模式提高写入速度"><a href="#使用多线程处理模式提高写入速度" class="headerlink" title="使用多线程处理模式提高写入速度"></a>使用多线程处理模式提高写入速度</h4><p>将对数据集进行拆分，规定每一个线程处理N条数据，并行写入Redis提高速度。</p>
<h4 id="执行时间表"><a href="#执行时间表" class="headerlink" title="执行时间表"></a>执行时间表</h4><table>
<thead>
<tr>
<th>数据量</th>
<th>方案说明</th>
<th>耗时（MS）</th>
</tr>
</thead>
<tbody><tr>
<td>320000</td>
<td>不使用redis pipeline模式</td>
<td>291070</td>
</tr>
<tr>
<td>320000</td>
<td>使用redis pipeline</td>
<td>25337</td>
</tr>
<tr>
<td>320000</td>
<td>使用 pipeline+多线程</td>
<td>9780</td>
</tr>
</tbody></table>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhangruipython.github.io/2020/07/01/presto%E5%88%9B%E5%BB%BAUDF%E5%87%BD%E6%95%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张睿">
      <meta itemprop="description" content="我的学习记录">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MyBlog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/07/01/presto%E5%88%9B%E5%BB%BAUDF%E5%87%BD%E6%95%B0/" class="post-title-link" itemprop="url">presto创建UDF函数</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-07-01 19:05:27 / 修改时间：20:23:46" itemprop="dateCreated datePublished" datetime="2020-07-01T19:05:27+08:00">2020-07-01</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="通过开发Plugin插件方式开发Presto-UDF"><a href="#通过开发Plugin插件方式开发Presto-UDF" class="headerlink" title="通过开发Plugin插件方式开发Presto  UDF"></a>通过开发Plugin插件方式开发Presto  UDF</h2><h3 id="Presto-插件机制为开发者提供了以下功能："><a href="#Presto-插件机制为开发者提供了以下功能：" class="headerlink" title="Presto 插件机制为开发者提供了以下功能："></a>Presto 插件机制为开发者提供了以下功能：</h3><ul>
<li>对接自定义存储系统</li>
<li>添加自定义数据类型</li>
<li>添加自定义处理函数</li>
<li>自定义权限控制</li>
</ul>
<h3 id="以下是开发Presto-UDF函数的步骤"><a href="#以下是开发Presto-UDF函数的步骤" class="headerlink" title="以下是开发Presto UDF函数的步骤"></a>以下是开发Presto UDF函数的步骤</h3><h4 id="1、逻辑代码开发"><a href="#1、逻辑代码开发" class="headerlink" title="1、逻辑代码开发"></a>1、逻辑代码开发</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">UdfDemo</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="title function_">UdfDemo</span><span class="params">()</span>&#123;&#125;</span><br><span class="line">    <span class="meta">@Description(&quot;两值相除&quot;)</span></span><br><span class="line">    <span class="meta">@ScalarFunction(value = &quot;divide&quot;)</span></span><br><span class="line">    <span class="meta">@SqlType(StandardTypes.DOUBLE)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">double</span> <span class="title function_">divide</span><span class="params">(<span class="meta">@SqlType(StandardTypes.DOUBLE)</span> <span class="type">double</span> num01,<span class="meta">@SqlType(StandardTypes.DOUBLE)</span> <span class="type">double</span> num02)</span>&#123;</span><br><span class="line">        <span class="type">double</span> <span class="variable">result</span> <span class="operator">=</span> num01/num02;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>注意点</strong></p>
<ul>
<li>@Description 是自定义方法注释</li>
<li>@ScalarFunction 是函数名称</li>
<li>@SqlType 是函数出参</li>
<li>@ScalarFunction 中函数名称需要和具体方法名称保持一致</li>
<li>方法入参需要添加 @SqlType 控制Java类型与Presto类型对应</li>
</ul>
<h4 id="2、插件编写"><a href="#2、插件编写" class="headerlink" title="2、插件编写"></a>2、插件编写</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DemoPlugin</span> <span class="keyword">implements</span> <span class="title class_">Plugin</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Set&lt;Class&lt;?&gt;&gt; getFunctions() &#123;</span><br><span class="line">        <span class="keyword">return</span>  ImmutableSet.&lt;Class&lt;?&gt;&gt;builder()</span><br><span class="line">                .add(UdfDemo.class)</span><br><span class="line">                .build();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>注意点</strong></p>
<ul>
<li>add所添加类即为对应的UDF方法类</li>
</ul>
<h4 id="3、服务注册"><a href="#3、服务注册" class="headerlink" title="3、服务注册"></a>3、服务注册</h4><ul>
<li>在src&#x2F;main&#x2F; 目录下创建以下文件夹resource&#x2F;META-INF&#x2F;services</li>
<li>在services文件夹中添加名称为 com.facebook.presto.spi.Plugin的文件</li>
<li>在 com.facebook.presto.spi.Plugin中添加实现的插件类名称，如图所示</li>
</ul>
<p><img src="https://i.loli.net/2020/07/01/GKquke9A7WZCEgV.png" alt="批注 2020-07-01 192624.png"></p>
<h4 id="4、发布jar包"><a href="#4、发布jar包" class="headerlink" title="4、发布jar包"></a>4、发布jar包</h4><ul>
<li>用maven 将项目打包（添加所有依赖）</li>
<li>在presto 安装目录下的plugin 文件夹中新建文件夹名称为UDF名称，将jar包放置于该文件夹中</li>
<li>重启presto服务</li>
</ul>
<h4 id="5、在cli端使用UDF"><a href="#5、在cli端使用UDF" class="headerlink" title="5、在cli端使用UDF"></a>5、在cli端使用UDF</h4><p><strong>如图所示</strong></p>
<p><img src="https://i.loli.net/2020/07/01/upA8LWI4mFQUi5Z.png" alt="批注 2020-07-01 193431.png"></p>
<h3 id="过程中可能出现的异常"><a href="#过程中可能出现的异常" class="headerlink" title="过程中可能出现的异常"></a>过程中可能出现的异常</h3><h4 id="presto-启动日志中出现-localfile-already-exist，原因maven打包中添加了-以下依赖"><a href="#presto-启动日志中出现-localfile-already-exist，原因maven打包中添加了-以下依赖" class="headerlink" title="presto 启动日志中出现 localfile already exist，原因maven打包中添加了 以下依赖"></a>presto 启动日志中出现 localfile already exist，原因maven打包中添加了 以下依赖</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;com.facebook.presto&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;presto-local-file&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;0.235.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhangruipython.github.io/2020/05/10/%E4%BD%BF%E7%94%A8spark-ml%E6%9E%84%E5%BB%BA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张睿">
      <meta itemprop="description" content="我的学习记录">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MyBlog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/10/%E4%BD%BF%E7%94%A8spark-ml%E6%9E%84%E5%BB%BA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/" class="post-title-link" itemprop="url">使用spark ml构建回归模型</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-05-10 14:26:39 / 修改时间：19:25:14" itemprop="dateCreated datePublished" datetime="2020-05-10T14:26:39+08:00">2020-05-10</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="使用Spark-MLlib模块进行机器学习模型训练"><a href="#使用Spark-MLlib模块进行机器学习模型训练" class="headerlink" title="使用Spark MLlib模块进行机器学习模型训练"></a>使用Spark MLlib模块进行机器学习模型训练</h1><hr>
<h2 id="使用模块介绍"><a href="#使用模块介绍" class="headerlink" title="使用模块介绍"></a>使用模块介绍</h2><p><strong>Spark MLlib是基于dataframe数据格式的机器学习模块，Spark ML支持基于Pipeline的数据流式模型训练，我们可以通过构建一个一个的stage模块组成Pipeline模块，进行模型训练。</strong></p>
<h2 id="建模流程："><a href="#建模流程：" class="headerlink" title="建模流程："></a>建模流程：</h2><p><strong>数据加载 &#x3D;&#x3D;&gt; 特征预处理 &#x3D;&#x3D;&gt; 算法初始化 &#x3D;&#x3D;&gt; 构建Pipeline处理器 &#x3D;&#x3D;&gt; 模型评估</strong></p>
<p><strong>完整代码位于</strong>：<a target="_blank" rel="noopener" href="https://github.com/zhangruipython/ETLPlatform/blob/master/SparkPlatform/src/main/java/com/application/ml/RegressionPipeline.java">https://github.com/zhangruipython/ETLPlatform/blob/master/SparkPlatform/src/main/java/com/application/ml/RegressionPipeline.java</a> </p>
<hr>
<h2 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h2><p><strong>由于旧的基于RDD数据格式进行模型训练的Spark MLlib已经弃用，所以统一将数据加载为dataframe数据格式</strong></p>
<p>由于数据源是CSV文件，所以需要注明head和分隔符，因为数据源数据并不大所以可以全部缓存在内存中，便于加速训练。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; dataDf = spark.read().format(<span class="string">&quot;csv&quot;</span>).option(<span class="string">&quot;header&quot;</span>,<span class="string">&quot;true&quot;</span>).option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;,&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>).load(<span class="string">&quot;D:/rongze/notebook/SCT___a___v4_prepared.csv&quot;</span>).cache();</span><br></pre></td></tr></table></figure>



<h2 id="特征预处理"><a href="#特征预处理" class="headerlink" title="特征预处理"></a>特征预处理</h2><p><strong>数据特征预处理步骤如下：</strong></p>
<h3 id="缺失值补充-x3D-x3D-gt-区分字符型和数值型变量-x3D-x3D-gt-字符型变量转为哑变量-x3D-x3D-gt-数值类型变量归一化-x3D-gt-数值型变量进行共线性检验-x3D-x3D-gt-特征变量合并"><a href="#缺失值补充-x3D-x3D-gt-区分字符型和数值型变量-x3D-x3D-gt-字符型变量转为哑变量-x3D-x3D-gt-数值类型变量归一化-x3D-gt-数值型变量进行共线性检验-x3D-x3D-gt-特征变量合并" class="headerlink" title="缺失值补充 &#x3D;&#x3D;&gt; 区分字符型和数值型变量 &#x3D;&#x3D;&gt; 字符型变量转为哑变量 &#x3D;&#x3D;&gt; 数值类型变量归一化 &#x3D;&gt; 数值型变量进行共线性检验 &#x3D;&#x3D;&gt; 特征变量合并"></a>缺失值补充 &#x3D;&#x3D;&gt; 区分字符型和数值型变量 &#x3D;&#x3D;&gt; 字符型变量转为哑变量 &#x3D;&#x3D;&gt; 数值类型变量归一化 &#x3D;&gt; 数值型变量进行共线性检验 &#x3D;&#x3D;&gt; 特征变量合并</h3><h3 id="区分字符型和数值型变量："><a href="#区分字符型和数值型变量：" class="headerlink" title="区分字符型和数值型变量："></a>区分字符型和数值型变量：</h3><p><strong>通过遍历找出字符类型变量和数值类型变量，同时去除label变量</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">String str = &quot;StringType&quot;;</span><br><span class="line">List&lt;String&gt; strColumns = new ArrayList&lt;String&gt;();</span><br><span class="line">dataDf.schema().foreach(s-&gt;&#123;</span><br><span class="line">            // 判断列属性</span><br><span class="line">            if(str.equals(s.dataType().toString()))&#123;</span><br><span class="line">                strColumns.add(s.name());</span><br><span class="line">            &#125;</span><br><span class="line">            return strColumns;</span><br><span class="line">        &#125;);</span><br><span class="line">List&lt;String&gt; numberColumns = Arrays.stream(dataDf.columns()).filter(item-&gt;!strColumns.contains(item)).collect(Collectors.toList());</span><br><span class="line">numberColumns.remove(&quot;SCT&quot;); //去除label变量</span><br></pre></td></tr></table></figure>

<h3 id="字符型变量进行OneHot编码转换："><a href="#字符型变量进行OneHot编码转换：" class="headerlink" title="字符型变量进行OneHot编码转换："></a>字符型变量进行OneHot编码转换：</h3><p><strong>字符类型变量转为哑变量的过程由两部构成：</strong></p>
<p><strong>（1）字符变量索引为对应的数值index，这里调用了Spark ML的String Indexer模块对字符变量进行索引，由于只能对单个元素进行索引，所以只能放在了循环中迭代处理，代码如下:</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">List&lt;String&gt; indexColumns = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        List&lt;String&gt; vocColumns = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (String strColumn : strColumns) &#123;</span><br><span class="line">            <span class="type">String</span> <span class="variable">columnIndex</span> <span class="operator">=</span> strColumn + <span class="string">&quot;_index&quot;</span>;</span><br><span class="line">            <span class="type">String</span> <span class="variable">columnVoc</span> <span class="operator">=</span> strColumn + <span class="string">&quot;_voc&quot;</span>;</span><br><span class="line">            indexColumns.add(columnIndex);</span><br><span class="line">            vocColumns.add(columnVoc);</span><br><span class="line">            <span class="type">StringIndexer</span> <span class="variable">indexer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringIndexer</span>().setInputCol(strColumn).setOutputCol(columnIndex);</span><br><span class="line">            dataDf = indexer.fit(dataDf).transform(dataDf);</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>

<p><strong>（2）字符索引数值类型变量OneHot处理，这里是调用了Spark ML中的 OneHotEncoderEstimator 模块，与字符变量索引为数值变量不同的是，这次操作没有将dataframe数据落地，而是通过生成Model构建Pipeline中的stage，代码如下</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">OneHotEncoderModel</span> <span class="variable">oneHotModel</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">OneHotEncoderEstimator</span>().setInputCols(indexColumns.toArray(<span class="keyword">new</span> <span class="title class_">String</span>[<span class="number">0</span>])).setOutputCols(vocColumns.toArray(<span class="keyword">new</span> <span class="title class_">String</span>[<span class="number">0</span>])).fit(dataDf);</span><br></pre></td></tr></table></figure>

<h3 id="特征变量合并"><a href="#特征变量合并" class="headerlink" title="特征变量合并"></a>特征变量合并</h3><p><strong>将所有特征变量合并为feature变量，生成stage</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">VectorAssembler vectorAssembler = new VectorAssembler().setInputCols(numberColumns.toArray(new String[0])).setOutputCol(&quot;features&quot;);</span><br></pre></td></tr></table></figure>



<p>注：由于元数据的特殊性，所以这次回归模型构建没有进行共线性检验，同时所有的缺失数据预处理已经在Dataiku平台上完成。</p>
<h2 id="算法初始化"><a href="#算法初始化" class="headerlink" title="算法初始化"></a>算法初始化</h2><p><strong>Spark MLlib模块支持大部分的回归算法，这次预测模型训练选用了梯度提升回归树算法</strong></p>
<p><strong>将算法进行初始化，设定label变量和feature变量以及迭代次数，构建Pipeline的stage</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">GBTRegressor</span> <span class="variable">gbt</span>  <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">GBTRegressor</span>().setLabelCol(<span class="string">&quot;SCT&quot;</span>).setFeaturesCol(<span class="string">&quot;features&quot;</span>).setMaxIter(<span class="number">10</span>);</span><br></pre></td></tr></table></figure>

<h2 id="构建Pipeline处理器"><a href="#构建Pipeline处理器" class="headerlink" title="构建Pipeline处理器"></a>构建Pipeline处理器</h2><p><strong>将各个模块stage传入Pipeline中</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Pipeline</span> <span class="variable">pipeline</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Pipeline</span>().setStages(<span class="keyword">new</span> <span class="title class_">PipelineStage</span>[]&#123;oneHotModel,vectorAssembler,gbt&#125;);</span><br></pre></td></tr></table></figure>

<p><strong>Pipeline模型训练</strong></p>
<p>将dataframe格式的元数据split为train data和test data进行模型训练</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt;[] splitDf = dataDf.randomSplit(new double[]&#123;0.8,0.2&#125;);</span><br><span class="line">PipelineModel pipelineModel = pipeline.fit(splitDf[0]);</span><br></pre></td></tr></table></figure>

<h2 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h2><p><strong>计算模型R2 Score</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; prediction = pipelineModel.transform(splitDf[1]);</span><br><span class="line">prediction.select(&quot;SCT&quot;,&quot;prediction&quot;,&quot;features&quot;).show(100);</span><br><span class="line">RegressionEvaluator evaluator = new RegressionEvaluator().setLabelCol(&quot;SCT&quot;).setMetricName(&quot;r2&quot;).setPredictionCol(&quot;prediction&quot;);</span><br><span class="line">System.out.println(evaluator.evaluate(prediction));</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="对使用Spark-ML中Pipeline进行模型训练的一些总结"><a href="#对使用Spark-ML中Pipeline进行模型训练的一些总结" class="headerlink" title="对使用Spark ML中Pipeline进行模型训练的一些总结"></a>对使用Spark ML中Pipeline进行模型训练的一些总结</h2><p><strong>1、我一开始并没有使用Pipeline流进行模型训练，而是基于训练数据，不断处理dataframe，每一步都将dataframe数据落地，这样导致的后果就是代码复杂，而且模型训练时间很长，整个流程耗时达到了1分多钟，这对比Sklearn实在是糟心，在使用Pipeline流式处理后，流程耗时只有2s左右，速度提升很大，使用Pipeline的关键在于不要去手动操作dataframe数据，整个Pipeline中应该只有一个数据入口，所有操作都要写成stage，这样要求我们必须熟悉Spark MLlib模块的API，所有在写代码之前需要仔细研究API入口，没有文档就需要我们去源码查阅。</strong></p>
<p><strong>2、Pipeline一个好处在于，整个Pipeline Model是可以持久化的，这样我们训练好模型后，只要load模型，传入数据就可以获取预测值，不需要再次进行数据预处理，方便了模型的应用。</strong></p>
<p><strong>3</strong>、<strong>最后是谈一下，Spark ML模块的一个缺点，Spark ML模块在Maven中的主要依赖包是</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;spark-mllib_2.12&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.4.5&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p><strong>但是当我在window环境中load持久化到本地的Pipeline模型时，会报包缺失的异常，当我直接submit jar包到Spark中是没问题的，这样带来的后果就是如果想将代码打包给别人用会有包缺失的异常，如果将所有依赖包都添加进来，这样jar会特别大，总而言之Spark ML不适合轻量级的单机版机器学习模型应用，如果使用JVM语言进行机器学习，可以使用smile平台。</strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhangruipython.github.io/2020/02/18/nvidia-docker%E9%85%8D%E7%BD%AEdarknet%E5%BA%94%E7%94%A8%E9%95%9C%E5%83%8F%E7%8E%AF%E5%A2%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张睿">
      <meta itemprop="description" content="我的学习记录">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MyBlog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/18/nvidia-docker%E9%85%8D%E7%BD%AEdarknet%E5%BA%94%E7%94%A8%E9%95%9C%E5%83%8F%E7%8E%AF%E5%A2%83/" class="post-title-link" itemprop="url">nvidia docker配置darknet应用镜像环境</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-02-18 10:24:31" itemprop="dateCreated datePublished" datetime="2020-02-18T10:24:31+08:00">2020-02-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-06-10 20:27:30" itemprop="dateModified" datetime="2020-06-10T20:27:30+08:00">2020-06-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="nvidia-docker配置GPU-深度学习镜像"><a href="#nvidia-docker配置GPU-深度学习镜像" class="headerlink" title="nvidia docker配置GPU 深度学习镜像"></a>nvidia docker配置GPU 深度学习镜像</h1><hr>
<h2 id="使用目标"><a href="#使用目标" class="headerlink" title="使用目标"></a>使用目标</h2><p><strong>通过nvidia docker搭建在宿主机上搭建装有cuda,cudnn,opencv,darkent的基础镜像，从而便于后续基于GPU目标识别应用的容器化部署和管理</strong></p>
<h2 id="安装流程"><a href="#安装流程" class="headerlink" title="安装流程"></a>安装流程</h2><h3 id="安装nvidia-docker"><a href="#安装nvidia-docker" class="headerlink" title="安装nvidia docker"></a>安装nvidia docker</h3><ul>
<li><p><strong>卸载nvidia-docker和其他GPU容器</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker volume ls -q -f driver=nvidia-docker | xargs -r -I&#123;&#125; -n1 docker ps -q -a -f volume=&#123;&#125; | xargs -r docker rm -f</span><br><span class="line">sudo apt-get purge -y nvidia-docker</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>添加package repositories</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \</span><br><span class="line">  sudo apt-key add -</span><br><span class="line">distribution=$(. /etc/os-release;echo $ID$VERSION_ID)</span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \</span><br><span class="line">  sudo tee /etc/apt/sources.list.d/nvidia-docker.list</span><br><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure>
</li>
<li><h3 id="安装-nvidia-docker2"><a href="#安装-nvidia-docker2" class="headerlink" title="安装 nvidia-docker2"></a>安装 nvidia-docker2</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install -y nvidia-docker2</span><br><span class="line">sudo pkill -SIGHUP dockerd</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="安装cuda基础镜像"><a href="#安装cuda基础镜像" class="headerlink" title="安装cuda基础镜像"></a>安装cuda基础镜像</h3><ul>
<li><p><strong>安装指定版本cuda</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-docker run --rm -ti nvidia/cuda:&#123;cuda版本&#125; nvcc --version</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="更新基础cuda镜像"><a href="#更新基础cuda镜像" class="headerlink" title="更新基础cuda镜像"></a>更新基础cuda镜像</h3><pre><code>更新cuda镜像的方式有很多，例如：通过DockerFile文件更新镜像，通过bash编辑镜像然后commit，我这里使用的方法是，先通过cuda镜像生成容器，通过编辑容器，将容器保存为新的镜像文件，这样的原因在于可以很方便的通过docker cap命令拷贝文件进入容器中
</code></pre>
<h4 id="预先准备"><a href="#预先准备" class="headerlink" title="预先准备"></a>预先准备</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1、更换apt安装镜像源</span><br><span class="line">sed -i &#x27;s#http://archive.ubuntu.com/#http://mirrors.tuna.tsinghua.edu.cn/#&#x27; /etc/apt/sources.list</span><br><span class="line">2、更新apt镜像源，修复包，安装python3 pip</span><br><span class="line">apt-get update -y --fix-missing &amp;&amp; apt-get install -y python3-pip python3-dev libsm6 libxext6 libxrender-dev --fix-missing</span><br><span class="line">3、安装依赖包</span><br><span class="line">apt-get install -y \</span><br><span class="line">    wget \</span><br><span class="line">    unzip \</span><br><span class="line">    ffmpeg \</span><br><span class="line">    git</span><br><span class="line"></span><br><span class="line"># Install cmake</span><br><span class="line">apt-get install -y build-essential</span><br><span class="line">apt-get install -y cmake</span><br></pre></td></tr></table></figure>



<h4 id="安装OpenCV"><a href="#安装OpenCV" class="headerlink" title="安装OpenCV"></a>安装OpenCV</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1、下载opencv源码包，opencv源码包有1个多G，可以在gitee上clone源码，速度很快</span><br><span class="line">git clone -b 3.4 https://gitee.com/mirrors/opencv.git</span><br><span class="line">2、安装依赖包</span><br><span class="line">sudo apt-get install libpng-dev</span><br><span class="line">sudo apt-get install libjpeg-dev</span><br><span class="line">sudo apt-get install libopenexr-dev</span><br><span class="line">sudo apt-get install libtiff-dev</span><br><span class="line">sudo apt-get install libwebp-dev</span><br><span class="line">3、在OpenCV中创建build文件夹</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">4、配置和编译</span><br><span class="line">cmake ../</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>

<h4 id="安装cudnn"><a href="#安装cudnn" class="headerlink" title="安装cudnn"></a>安装cudnn</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1、下载与cuda版本对应的cudnn</span><br><span class="line">2、将cudnn拷贝至容器指定目录下 docker cp &#123;cudnn文件目录&#125; &#123;容器目录&#125;</span><br><span class="line">3、解压cudnn文件</span><br><span class="line">4、sudo cp cuda/include/cudnn.h /usr/local/cuda/include</span><br><span class="line">   sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64</span><br><span class="line">   sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure>



<h4 id="编译darknet"><a href="#编译darknet" class="headerlink" title="编译darknet"></a>编译darknet</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1、下载ABdarknet源码</span><br><span class="line">git clone https://github.com/AlexeyAB/darknet</span><br><span class="line">2、修改Makefile文件</span><br><span class="line">GPU=1</span><br><span class="line">CUDNN=1</span><br><span class="line">OPENCV=1</span><br><span class="line">3、编译</span><br><span class="line">make</span><br></pre></td></tr></table></figure>

<h4 id="根据基础镜像创建容器"><a href="#根据基础镜像创建容器" class="headerlink" title="根据基础镜像创建容器"></a>根据基础镜像创建容器</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-docker run -it name &#123;容器名称&#125; &#123;镜像名称&#125; &#123;容器位置&#125;</span><br></pre></td></tr></table></figure>

<p><strong>注意：1、一定要用nvidia-docker创建容器，否者会出现cudnn not found的问题</strong></p>
<p><strong>2、调用darknet时如果出现 error while loading shared libraries:libopencv_highgui.so.3.4: cannot open shared object file的问题，编辑 &#x2F;etc&#x2F;ld.so.conf文件，加上include &#x2F;etc&#x2F;ld.so.conf.d&#x2F;*.con，然后执行ldconfig，在darknet目录下make clean后重新make</strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhangruipython.github.io/2019/08/06/spacy+doccano%20NER%E6%B5%81%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张睿">
      <meta itemprop="description" content="我的学习记录">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MyBlog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/08/06/spacy+doccano%20NER%E6%B5%81%E7%A8%8B/" class="post-title-link" itemprop="url">spacy+doccano 中文NLP流程</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2019-08-06 22:56:55 / 修改时间：22:56:44" itemprop="dateCreated datePublished" datetime="2019-08-06T22:56:55+08:00">2019-08-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="spacy-doccano-中文NLP流程"><a href="#spacy-doccano-中文NLP流程" class="headerlink" title="spacy+doccano 中文NLP流程"></a>spacy+doccano 中文NLP流程</h2><h3 id="使用工具介绍"><a href="#使用工具介绍" class="headerlink" title="使用工具介绍"></a>使用工具介绍</h3><p>spacy是一个工业级python自然语言处理包，支持自然语言文本分析、命名实体识别、词性标注、依存句法分析等功能。spacy2.0之后通过引入结巴分词，添加了对中文NLP的支持，不过在使用spacy进行中文自然语言处理时有许多需要注意的地方。</p>
<p>doccano是一个在GitHub上开源的可视化实体标注工具，支持自定义实体标签，文本实体标注，导出标注数据为jsonl（JSON Lines文件，结构化数据，用于管道文件传输）。在中文NPL处理中doccano主要作用是提供中文NER训练参数。</p>
<hr>
<h2 id="doccano使用过程"><a href="#doccano使用过程" class="headerlink" title="doccano使用过程"></a>doccano使用过程</h2><h3 id="在doccano上对语句进行命名实体标签标注"><a href="#在doccano上对语句进行命名实体标签标注" class="headerlink" title="在doccano上对语句进行命名实体标签标注"></a>在doccano上对语句进行命名实体标签标注</h3><ul>
<li>自定义实体标签</li>
<li>选择实体设定实体标签</li>
</ul>
<h3 id="将doccano上数据导出为json数据"><a href="#将doccano上数据导出为json数据" class="headerlink" title="将doccano上数据导出为json数据"></a>将doccano上数据导出为json数据</h3><ul>
<li>export data为json格式</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;id&quot;: 1, &quot;text&quot;: &quot;The Hitchhiker&#x27;s Guide to the Galaxy (sometimes referred to as HG2G, HHGTTGor H2G2) is a comedy science fiction series created by Douglas Adams. Originally a radio comedy broadcast on BBC Radio 4 in 1978, it was later adapted to other formats, including stage shows, novels, comic books, a 1981 TV series, a 1984 video game, and 2005 feature film.&quot;, &quot;annotations&quot;: [], &quot;meta&quot;: &#123;&#125;, &quot;annotation_approver&quot;: null&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="解析json数据为TRAIN-DATA-数据格式"><a href="#解析json数据为TRAIN-DATA-数据格式" class="headerlink" title="解析json数据为TRAIN_DATA 数据格式"></a>解析json数据为TRAIN_DATA 数据格式</h3><ul>
<li>解析json格式数据</li>
<li>抽取数据中的labels</li>
</ul>
<h3 id="在现有模型中训练TRAIN-DATA，添加实体类别，更新模型"><a href="#在现有模型中训练TRAIN-DATA，添加实体类别，更新模型" class="headerlink" title="在现有模型中训练TRAIN_DATA，添加实体类别，更新模型"></a>在现有模型中训练TRAIN_DATA，添加实体类别，更新模型</h3><ul>
<li>在模型中添加实体label</li>
<li>循环训练model</li>
</ul>
<h3 id="对模型测试，查看模型训练效果，对于当前模型未能准确识别的实体，在doccano上更新标注"><a href="#对模型测试，查看模型训练效果，对于当前模型未能准确识别的实体，在doccano上更新标注" class="headerlink" title="对模型测试，查看模型训练效果，对于当前模型未能准确识别的实体，在doccano上更新标注"></a>对模型测试，查看模型训练效果，对于当前模型未能准确识别的实体，在doccano上更新标注</h3><ul>
<li>与训练语句相同句法语句使用模型划分出实体label和实体名称</li>
<li>对照找出当前模型划分错误之处</li>
<li>在doccano调整训练语句</li>
</ul>
<h2 id="spacy的使用及安装过程"><a href="#spacy的使用及安装过程" class="headerlink" title="spacy的使用及安装过程"></a>spacy的使用及安装过程</h2><h3 id="spacy在centos7环境下的安装"><a href="#spacy在centos7环境下的安装" class="headerlink" title="spacy在centos7环境下的安装"></a>spacy在centos7环境下的安装</h3><p>在使用pip安装spacy时，因为面对的是中文环境，spacy2.0以后通过引入jieba分词添加了对中文的支持，但是还是有较多的坑。</p>
<p>如果要使用中文模型，目前的安装环境是python3.6+spacy2.0.1，如果安装了最高版本的spacy2.1，在使用中文模型时会出现python re正则包出错的情况。</p>
<p>中文模型的下载地址：<a target="_blank" rel="noopener" href="https://github.com/howl-anderson/Chinese_models_for_SpaCy/releases">https://github.com/howl-anderson/Chinese_models_for_SpaCy/releases</a> </p>
<ul>
<li>在使用中文模型时需要控制 msgpack-numpy&#x3D;&#x3D;0.4.4.2  不然会出现 (TypeError:encoding)编码的错误</li>
</ul>
<p>安装好spacy和下载好中文模型之后，就是在spacy中配置中文环境，如何配置中文环境参考<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/0dab70cb540e">https://www.jianshu.com/p/0dab70cb540e</a> </p>
<p>配置好中文环境后，由于spacy使用的jieba分词模式是最普通的模式，遇到行业内专业词汇很难准确分词，这就需要我们使用jieba的自定义词典分词功能。</p>
<ul>
<li>使用自定义分词字典</li>
</ul>
<p>先自定义一个TXT文档，用于标注自定义词块、词性、词频</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">拿铁 3 i</span><br><span class="line">摩卡 3 i</span><br><span class="line">全自动咖啡机 3 i</span><br><span class="line">半自动咖啡机 3 i</span><br><span class="line">美式（传统滴滤式）咖啡机 100 i</span><br><span class="line">意式咖啡机 19 i</span><br><span class="line">胶囊咖啡机 3 i</span><br><span class="line">月房租 4 i</span><br><span class="line">卡布奇洛 4 i</span><br></pre></td></tr></table></figure>

<p>代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import jieba</span><br><span class="line">import jieba.posseg as pseg</span><br><span class="line">import jieba.analyse as anls</span><br><span class="line"></span><br><span class="line">doc = &quot;&quot;&quot;我在江北新区研创园喝咖啡，喝的是卡布奇洛&quot;&quot;&quot;</span><br><span class="line">doc01 = &quot;&quot;&quot;有一台全自动咖啡机出现故障了&quot;&quot;&quot;</span><br><span class="line">doc02 = &quot;&quot;&quot;江北新区研创园的月房租是10000人民币&quot;&quot;&quot;</span><br><span class="line">jieba.load_userdict(&quot;demo.txt&quot;)</span><br><span class="line"></span><br><span class="line"># 全模式</span><br><span class="line">seg_list = jieba.cut(doc)</span><br><span class="line">seg_list01 = jieba.cut(doc01)</span><br><span class="line">seg_list02 = jieba.cut(doc02)</span><br><span class="line">print(type(seg_list))</span><br><span class="line">print(&quot;/&quot;.join(seg_list))</span><br><span class="line">print(&quot;/&quot;.join(seg_list01))</span><br><span class="line">print(&quot;/&quot;.join(seg_list02))</span><br></pre></td></tr></table></figure>

<p>效果如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">我/在/江北/新区/研创园/喝咖啡/，/喝/的/是/卡布奇洛</span><br><span class="line">有/一台/全自动咖啡机/出现/故障/了</span><br><span class="line">江北/新区/研创园/的/月房租/是/10000/人民币</span><br></pre></td></tr></table></figure>

<ul>
<li>因为当前目标是在文档中找出指定实体类型的实体，如找出实体类型为”法定代表人”的所有实体，在spacy中现有的实体类型并不能满足需求，所以这里需要使用命名实体标注和NER（命名实体识别），自定义实体标注的工作由doccano完成，在获取到训练数据后，需要将其训练至spacy的中文模型中。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">训练一个新的命名实体识别</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function</span><br><span class="line"><span class="comment"># from nlp_app.spacy_demo.data_conversion import conversion</span></span><br><span class="line"><span class="keyword">import</span> plac</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line"><span class="keyword">from</span> spacy.util <span class="keyword">import</span> minibatch, compounding</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析json数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conversion</span>(<span class="params">path</span>):</span><br><span class="line">    doc = <span class="built_in">open</span>(path, <span class="string">&quot;r&quot;</span>)</span><br><span class="line">    train_data = []</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># all_text = doc.read()</span></span><br><span class="line">        lines = doc.readlines()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(lines) != <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">for</span> all_text <span class="keyword">in</span> lines:</span><br><span class="line">                <span class="built_in">print</span>(<span class="built_in">type</span>(json.loads(all_text)))</span><br><span class="line">                all_text = json.loads(all_text)</span><br><span class="line">                text_content = all_text[<span class="string">&quot;text&quot;</span>]</span><br><span class="line">                text_entities = all_text[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line">                entity_list = []</span><br><span class="line">                <span class="keyword">for</span> entity <span class="keyword">in</span> text_entities:</span><br><span class="line">                    entity = <span class="built_in">tuple</span>(entity)</span><br><span class="line">                    entity_list.append(entity)</span><br><span class="line">                data = (text_content, &#123;<span class="string">&quot;entities&quot;</span>: entity_list&#125;)</span><br><span class="line">        train_data.append(data)</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        doc.close()</span><br><span class="line">    <span class="keyword">return</span> train_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TRAIN_DATA = conversion(<span class="string">&quot;/home/hadoop/Documents/train_data.json1&quot;</span>)</span><br><span class="line"><span class="comment"># 获取TRAIN_DATA 中的label</span></span><br><span class="line">label_list = []</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> TRAIN_DATA:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(data) == <span class="number">2</span>:</span><br><span class="line">        entities = data[<span class="number">1</span>][<span class="string">&quot;entities&quot;</span>]</span><br><span class="line">        <span class="keyword">for</span> entity <span class="keyword">in</span> entities:</span><br><span class="line">            label_list.append(entity[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@plac.annotations(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="meta">    model=(<span class="params"><span class="string">&quot;Model name. Defaults to blank &#x27;en&#x27; model.&quot;</span>, <span class="string">&quot;option&quot;</span>, <span class="string">&quot;m&quot;</span>, <span class="built_in">str</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    output_dir=(<span class="params"><span class="string">&quot;Optional output directory&quot;</span>, <span class="string">&quot;option&quot;</span>, <span class="string">&quot;o&quot;</span>, Path</span>),</span></span></span><br><span class="line"><span class="params"><span class="meta">    n_iter=(<span class="params"><span class="string">&quot;Number of training iterations&quot;</span>, <span class="string">&quot;option&quot;</span>, <span class="string">&quot;n&quot;</span>, <span class="built_in">int</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="meta"></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">model=<span class="string">&quot;zh_core_2.0.3&quot;</span>, output_dir=<span class="string">&quot;/usr/local/lib/python3.6/dist-packages/spacy/data/zh_core_2.0.3&quot;</span>,</span></span><br><span class="line"><span class="params">         n_iter=<span class="number">100</span></span>):</span><br><span class="line">    random.seed(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">if</span> model <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        nlp = spacy.load(model)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Loaded model &#x27;%s&#x27;&quot;</span> % model)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        nlp = spacy.blank(<span class="string">&quot;en&quot;</span>)  <span class="comment"># 选择语言创建模型</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Created blank &#x27;en&#x27; model&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;ner&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> nlp.pipe_names:</span><br><span class="line">        ner = nlp.create_pipe(<span class="string">&quot;ner&quot;</span>)</span><br><span class="line">        nlp.add_pipe(ner)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ner = nlp.get_pipe(<span class="string">&quot;ner&quot;</span>)</span><br><span class="line">    <span class="comment"># 命名实体识别添加标签</span></span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> label_list:</span><br><span class="line">        ner.add_label(label)</span><br><span class="line">   </span><br><span class="line">    move_names = <span class="built_in">list</span>(ner.move_names)</span><br><span class="line">    other_pipes = [pipe <span class="keyword">for</span> pipe <span class="keyword">in</span> nlp.pipe_names <span class="keyword">if</span> pipe != <span class="string">&quot;ner&quot;</span>]</span><br><span class="line">    <span class="keyword">with</span> nlp.disable_pipes(*other_pipes):  <span class="comment"># 训练模型</span></span><br><span class="line">        sizes = compounding(<span class="number">1.0</span>, <span class="number">4.0</span>, <span class="number">1.001</span>)</span><br><span class="line">        <span class="keyword">for</span> itn <span class="keyword">in</span> <span class="built_in">range</span>(n_iter):</span><br><span class="line">            random.shuffle(TRAIN_DATA)</span><br><span class="line">            batches = minibatch(TRAIN_DATA, size=sizes)</span><br><span class="line">            losses = &#123;&#125;  <span class="comment"># 训练损失</span></span><br><span class="line">            <span class="keyword">for</span> batch <span class="keyword">in</span> batches:</span><br><span class="line">                texts, annotations = <span class="built_in">zip</span>(*batch)</span><br><span class="line">                nlp.update(texts, annotations, drop=<span class="number">0.35</span>, losses=losses)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Losses&quot;</span>, losses)</span><br><span class="line">    test_text = TRAIN_DATA[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># doc = nlp(test_text)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Entities in &#x27;%s&#x27;&quot;</span> % test_text)</span><br><span class="line">    <span class="comment"># for ent in doc.ents:</span></span><br><span class="line">    <span class="comment">#     print(ent.label_, ent.text)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存模型至路径</span></span><br><span class="line">    <span class="keyword">if</span> output_dir <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        output_dir = Path(output_dir)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> output_dir.exists():</span><br><span class="line">            output_dir.mkdir()</span><br><span class="line">        <span class="comment"># nlp.meta[&quot;name&quot;] = new_model_name</span></span><br><span class="line">        nlp.to_disk(output_dir)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Saved model to&quot;</span>, output_dir)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 测试模型效果</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Loading from&quot;</span>, output_dir)</span><br><span class="line">        nlp2 = spacy.load(output_dir)</span><br><span class="line">        <span class="keyword">assert</span> nlp2.get_pipe(<span class="string">&quot;ner&quot;</span>).move_names == move_names</span><br><span class="line">        doc2 = nlp2(test_text)</span><br><span class="line">        <span class="keyword">for</span> ent <span class="keyword">in</span> doc2.ents:</span><br><span class="line">            <span class="built_in">print</span>(ent.label_, ent.text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    plac.call(main)</span><br></pre></td></tr></table></figure>

<p>通过不断强化训练，将自定义实体与文本间句法关系写入中文模型中。在模型训练中有一个值得重视的问题就是深度学习的<strong>灾难性遗忘</strong>问题，而这个问题在中文模型中尤为明显，模型训练好之后，可以对与训练数据类似句法模板的文档进行命名实体识别，而且准确率较高，但是一旦在该模型基础上再一次训练数据，那么之前所训练的内容将被遗忘，也就是说所有模型训练都是一次性的，不可重复。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhangruipython.github.io/2019/04/30/Neo4j%E7%9A%84%E8%AE%A4%E7%9F%A5%E4%B8%8E%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张睿">
      <meta itemprop="description" content="我的学习记录">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MyBlog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/04/30/Neo4j%E7%9A%84%E8%AE%A4%E7%9F%A5%E4%B8%8E%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">Neo4j的认知与学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2019-04-30 15:29:20 / 修改时间：16:31:38" itemprop="dateCreated datePublished" datetime="2019-04-30T15:29:20+08:00">2019-04-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>最近接触了知识图谱，并开始学习了Neo4j作为知识图谱的工具入门</p>
<h2 id="什么是知识图谱？"><a href="#什么是知识图谱？" class="headerlink" title="什么是知识图谱？"></a>什么是知识图谱？</h2><p>从一个简单的例子入手：在Google搜索引擎里输入“Who is the wife of Bill Gates?”，我们直接可以得到答案-“Melinda Gates”。这是因为我们在系统层面上已经创建好了一个包含“Bill Gates”和“Melinda Gates”的实体以及他俩之间关系的知识库。所以，当我们执行搜索的时候，就可以通过关键词提取（”Bill Gates”, “Melinda Gates”, “wife”）以及知识库上的匹配可以直接获得最终的答案。这种搜索方式跟传统的搜索引擎是不一样的，一个传统的搜索引擎它返回的是网页、而不是最终的答案，所以就多了一层用户自己筛选并过滤信息的过程。</p>
<p>这就是一种KBQA模式的知识图谱应用，所以知识图谱本质上就是语义网络的知识库，从实际应用的角度出发可以认为知识图谱就是多关系图。在知识图谱中，我们通常使用”实体”来表示图里的节点、用”关系”表示图中边。</p>
<h2 id="如何建立知识图谱"><a href="#如何建立知识图谱" class="headerlink" title="如何建立知识图谱"></a>如何建立知识图谱</h2><p>在建立知识图谱中我们选取的工具是Neo4j图形数据库，在建立知识图谱中最为关键的点在于：设定关系。有别于传统的关系型数据库，图数据库中一切基于关系。</p>
<h2 id="图数据库和传统关系型数据库的区别"><a href="#图数据库和传统关系型数据库的区别" class="headerlink" title="图数据库和传统关系型数据库的区别"></a>图数据库和传统关系型数据库的区别</h2><p>传统的关系型数据库在设计时都应该满足三范式</p>
<hr>
<p>回顾一下三范式</p>
<p>第一范式：确保每列的原子性</p>
<p>第二范式：确保表中每个字段都和主键有关</p>
<p>第三范式：确保每列都和主键直接相关而不是间接相关</p>
<hr>
<p>而在图数据库中每一个节点有自己的key-value属性，一群相同属性的节点就构成了label（标签），节点和节点之间通过关系相连接，relationship（关系）也有自己的key-value属性，同时在图数据中所有关系都是有向的。</p>
<p>将图数据库与传统关系型数据库比较可以认为，关系型数据库中的实体表就是图数据库中label，实体表中的每一条记录对应着图数据库中的一个节点，关系型数据库表与表之间的join关系就是图数据库中的relationship。</p>
<h2 id="Neo4j的使用"><a href="#Neo4j的使用" class="headerlink" title="Neo4j的使用"></a>Neo4j的使用</h2><h3 id="将关系型数据库数据转移到Neo4j中"><a href="#将关系型数据库数据转移到Neo4j中" class="headerlink" title="将关系型数据库数据转移到Neo4j中"></a>将关系型数据库数据转移到Neo4j中</h3><p>转移数据，我们使用的是Neo4j ETL 工具，它支持通过jdbc连接不同类型的数据库，将数据库中表导入Neo4j中，同时它可以自动判别关系型数据库中表与表的join关系，并在Neo4j中将其转化为relationship。</p>
<p>但是在使用Neo4j ETL 工具时有几点需要注意：</p>
<ul>
<li>千万别在Windows上操作！！！ 由于Neo4j只支持以csv格式大批次导入数据，所以Neo4j ETL在本质上还是将关系型数据库中的表转为csv缓存在本地，然后在将csv导入Neo4j中，问题就出现在缓存这一步上，数据一旦在Windows上落地，Windows就会将UTF-8格式的数据转为UTF-8+BOM格式数据，也就是加了三个空字符，UTF-8+BOM的编码格式数据在导入Neo4j时会出现乱码。</li>
<li>Neo4j ETL在做表与表之间关系处理上，是不会辨别关系方向的，很多时候关系方向是乱的，而在图数据中关系方向很重要，所以不建议使用表与表之间的join在图数据库中建立关系，还是要手动建立关系，指明方向。</li>
</ul>
<h3 id="一些常用的cypher语句"><a href="#一些常用的cypher语句" class="headerlink" title="一些常用的cypher语句"></a>一些常用的cypher语句</h3><p>删除关系，删除节点</p>
<p>MATCH (n:BC_Person)-[r]-() DELETE n,r</p>
<p>创建关系</p>
<p>match (a:LastSiteHangjiang),(b:PersonMessage) where a.lastType &#x3D; “写字楼” create (a)-[r:is_contain]-&gt;(b) return r</p>
<p>查询关系</p>
<p>match p&#x3D;(PersonMessage)-[r:is_contain]-&gt;() return p</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhangruipython.github.io/2019/04/20/postgres+pyspark%E8%AE%A1%E7%AE%97%E5%89%8D100%E6%9C%80%E7%9F%AD%E8%B7%9D%E7%A6%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张睿">
      <meta itemprop="description" content="我的学习记录">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MyBlog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/04/20/postgres+pyspark%E8%AE%A1%E7%AE%97%E5%89%8D100%E6%9C%80%E7%9F%AD%E8%B7%9D%E7%A6%BB/" class="post-title-link" itemprop="url">使用pyspark计算一点和周围距离最近的前100点位置</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-04-20 16:56:39" itemprop="dateCreated datePublished" datetime="2019-04-20T16:56:39+08:00">2019-04-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-04-30 16:36:50" itemprop="dateModified" datetime="2019-04-30T16:36:50+08:00">2019-04-30</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="使用pyspark计算一点和周围距离最近的前100点位置"><a href="#使用pyspark计算一点和周围距离最近的前100点位置" class="headerlink" title="使用pyspark计算一点和周围距离最近的前100点位置"></a>使用pyspark计算一点和周围距离最近的前100点位置</h2><h3 id="业务场景"><a href="#业务场景" class="headerlink" title="业务场景"></a>业务场景</h3><h5 id="有一批地理位置存储在postgres数据库表中为表1，有另一张基础表覆盖了地区所有的地理位置为表2，现在需要计算出表1中所有位置最近的前100个地点，并进行排序，最终在数据库中生成一张新表，表中包含目标地点的所有topN最近点的hash"><a href="#有一批地理位置存储在postgres数据库表中为表1，有另一张基础表覆盖了地区所有的地理位置为表2，现在需要计算出表1中所有位置最近的前100个地点，并进行排序，最终在数据库中生成一张新表，表中包含目标地点的所有topN最近点的hash" class="headerlink" title="有一批地理位置存储在postgres数据库表中为表1，有另一张基础表覆盖了地区所有的地理位置为表2，现在需要计算出表1中所有位置最近的前100个地点，并进行排序，最终在数据库中生成一张新表，表中包含目标地点的所有topN最近点的hash"></a>有一批地理位置存储在postgres数据库表中为表1，有另一张基础表覆盖了地区所有的地理位置为表2，现在需要计算出表1中所有位置最近的前100个地点，并进行排序，最终在数据库中生成一张新表，表中包含目标地点的所有topN最近点的hash</h5><h3 id="使用工具"><a href="#使用工具" class="headerlink" title="使用工具"></a>使用工具</h3><ul>
<li>pyspark</li>
<li>postgres</li>
</ul>
<h3 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h3><p>由于数据量过大所以需要将数据放在spark上运行，使用spark的dataframe数据格式进行数据处理比较便捷（spark2.0已经使用dataframe替代rdd）</p>
<ul>
<li><p>通过spark jdbc连接数据库，执行SQL生成dataframe格式数据，其中关键点在于如何通过SQL计算出topN距离（一开始我准备通过for循环，在代码中遍历求值，后来发现效率太低，而且需要通过spark udf将方法注册进spark），通过SQL进行计算范围两个步骤（1）计算出距离（2）对距离进行排序，排序时需要显性表示降序还是升序（3）对数据进行筛选</p>
<hr>
</li>
</ul>
<p>​       以下是SQL，其中txpop是基础地理信息表，test_by_zhangrui是目标地理信息表，由于不能使用group by聚合函数，             —– 所以换了一种方式：</p>
<p>​       </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> a.choose_site_hash,a.filtrate_site_hash,a.distance <span class="keyword">FROM</span></span><br><span class="line">(<span class="keyword">SELECT</span></span><br><span class="line">	tmp2.hash1 <span class="keyword">as</span> choose_site_hash,</span><br><span class="line">	txpop.grid_hash <span class="keyword">as</span> filtrate__site_hash,</span><br><span class="line">	st_distance (tmp2.geo1 :: geography, txpop.geo :: geography) <span class="keyword">as</span> distance,</span><br><span class="line">	 <span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">by</span> tmp2.hash1 <span class="keyword">ORDER</span> <span class="keyword">BY</span> (st_distance (tmp2.geo1 :: geography, txpop.geo :: geography)) <span class="keyword">ASC</span>) <span class="keyword">as</span> n</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">	(</span><br><span class="line">	<span class="keyword">SELECT</span></span><br><span class="line">		grid_hash <span class="keyword">AS</span> hash1,</span><br><span class="line">		geo <span class="keyword">AS</span> geo1 </span><br><span class="line">	<span class="keyword">FROM</span></span><br><span class="line">		test_by_zhangrui </span><br><span class="line">	) tmp2,</span><br><span class="line">	txpop </span><br><span class="line">) a</span><br><span class="line"><span class="keyword">WHERE</span> n<span class="operator">&lt;=</span><span class="number">10</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<hr>
<ul>
<li>注册spark （master参数表明spark是运行在本机还是集群上，以及配置多少个节点），通过spark jdbc驱动连接数据库，并执行SQL（详情可看官网：<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/sql-data-sources-jdbc.html">http://spark.apache.org/docs/latest/sql-data-sources-jdbc.html</a>）</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def get_dataframe_jdbc(a):</span><br><span class="line">    spark = SparkSession \</span><br><span class="line">        .builder \</span><br><span class="line">        .appName(&quot;Python Spark SQL basic example&quot;) \</span><br><span class="line">        .master(&quot;local[*]&quot;) \</span><br><span class="line">        .getOrCreate()</span><br><span class="line">    jdbcDF = spark.read \</span><br><span class="line">        .format(&quot;jdbc&quot;) \</span><br><span class="line">        .option(&quot;url&quot;, &quot;jdbc:postgresql://*****&quot;) \</span><br><span class="line">        .option(&quot;dbtable&quot;, a) \</span><br><span class="line">        .option(&quot;user&quot;, &quot;*****&quot;) \</span><br><span class="line">        .option(&quot;password&quot;, &quot;*****&quot;) \</span><br><span class="line">        .load()</span><br><span class="line">    return jdbcDF</span><br></pre></td></tr></table></figure>

<hr>
<ul>
<li>执行SQL后，spark返回一个spark dataframe格式数据，我们需要将dataframe并入数据库做持久化</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def write_dataframe_jdbc(table_name, result):</span><br><span class="line">    result.write \</span><br><span class="line">        .format(&quot;jdbc&quot;) \</span><br><span class="line">        .option(&quot;url&quot;, &quot;jdbc:postgresql://******&quot;) \</span><br><span class="line">        .option(&quot;dbtable&quot;, table_name) \</span><br><span class="line">        .option(&quot;user&quot;, &quot;*******&quot;) \</span><br><span class="line">        .option(&quot;password&quot;, &quot;*******&quot;) \</span><br><span class="line">        .save()</span><br><span class="line">    print(&quot;表生成完毕&quot;)</span><br></pre></td></tr></table></figure>

<ul>
<li>最后我们可以在数据库中得到我们想要的表（目前spark所有配置都是默认配置没有优化，包括SQL也没有优化，待日后进行优化）</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://zhangruipython.github.io/2019/04/20/pyspark-udf%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="张睿">
      <meta itemprop="description" content="我的学习记录">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MyBlog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/04/20/pyspark-udf%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0/" class="post-title-link" itemprop="url">pyspark udf自定义函数</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2019-04-20 16:56:39 / 修改时间：16:58:02" itemprop="dateCreated datePublished" datetime="2019-04-20T16:56:39+08:00">2019-04-20</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="pyspark-udf自定义函数"><a href="#pyspark-udf自定义函数" class="headerlink" title="pyspark udf自定义函数"></a>pyspark udf自定义函数</h2><h3 id="spark-udf的定义："><a href="#spark-udf的定义：" class="headerlink" title="spark udf的定义："></a>spark udf的定义：</h3><h4 id="UDF-User-defined-functions-UDFs-即用户自定义函数，在Spark-Sql的开发中十分常用，UDF对表中的每一行进行函数处理，返回新的值。"><a href="#UDF-User-defined-functions-UDFs-即用户自定义函数，在Spark-Sql的开发中十分常用，UDF对表中的每一行进行函数处理，返回新的值。" class="headerlink" title="UDF(User-defined functions, UDFs),即用户自定义函数，在Spark Sql的开发中十分常用，UDF对表中的每一行进行函数处理，返回新的值。"></a>UDF(User-defined functions, UDFs),即用户自定义函数，在Spark Sql的开发中十分常用，UDF对表中的每一行进行函数处理，返回新的值。</h4><h3 id="实际应用："><a href="#实际应用：" class="headerlink" title="实际应用："></a>实际应用：</h3><ul>
<li>定义一个函数</li>
</ul>
<p>这是一个普通的判断函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_bad_point</span>(<span class="params">grid01_order_index, grid02_order_index, grid01_hash, grid02_hash</span>):</span><br><span class="line">    <span class="keyword">if</span> grid01_order_index &lt; grid02_order_index:</span><br><span class="line">        <span class="keyword">return</span> grid02_hash</span><br><span class="line">    <span class="keyword">elif</span> grid02_order_index &lt; grid01_order_index:</span><br><span class="line">        <span class="keyword">return</span> grid01_hash</span><br></pre></td></tr></table></figure>

<ul>
<li>将函数注册为spark udf</li>
</ul>
<p>这里需要表明方法的返回值类型</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_bad_udf = udf(get_bad_point, StringType())</span><br></pre></td></tr></table></figure>

<ul>
<li>结合spark SQL自带方法使用</li>
</ul>
<p>向udf函数传入spark dataframe中列名作为参数</p>
<p>alias为udf返回的结果生成别名</p>
<p>distinct()是对dataframe中数据进行去重</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">unmatched_point_df = unmatched_distance_point.select(</span><br><span class="line">       get_bad_udf(<span class="string">&quot;grid01_order_index&quot;</span>, <span class="string">&quot;grid02_order_index&quot;</span>, <span class="string">&quot;grid01_hash&quot;</span>,</span><br><span class="line">                   <span class="string">&quot;grid02_hash&quot;</span>).alias(<span class="string">&quot;bad_point_hash&quot;</span>)).distinct()</span><br></pre></td></tr></table></figure>

<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>pyspark的使用可以大大降低编写spark查询的代码量，结合udf函数的使用使我们操作spark SQL时不需要写大量的SQL脚本，由于spark SQL为我们封装了许多的操作函数，比如：过滤filter，两个dataframe之间取差集exceptAll  等等</p>
<p>所以我们编写spark 程序代码量不应该很多，如果代码量很多出现了大量的for循环，if判断等那么我们的代码就需要优化，原因就是所有的SQL都是循环+判断，所以可以用spark SQL就用spark SQL，最好不要手写循环判断。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">张睿</p>
  <div class="site-description" itemprop="description">我的学习记录</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">张睿</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
